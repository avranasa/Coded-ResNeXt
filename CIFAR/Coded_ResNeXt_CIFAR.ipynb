{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coded-ResNeXt_CIFAR.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8fMy+gFqgePskKkTbDE/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avranasa/Coded-ResNeXt/blob/main/Coded_ResNeXt_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ie-iEFQiI_y"
      },
      "outputs": [],
      "source": [
        "#We had troubles on running with Colab TPU accelarator and different version of pytorch.\n",
        "#If prompted/printed after running this cell, press the button to RESTART RUNTIME \n",
        "!pip install timm\n",
        "!apt-get install -q subversion\n",
        "!svn checkout https://github.com/avranasa/Coded-ResNeXt.git/trunk/CIFAR /content/CIFAR\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu  torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q torchtext==0.9.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import os, sys, timm\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "sys.path.insert(0, '/content/CIFAR') \n",
        "from NN_ResNeXt import Net_ResNext, DataModule\n",
        "from NN_modules import ResNeXt_block\n",
        "\n",
        "'''\n",
        "For the architectures described below the notation is the following:\n",
        "    -'conv3_in3_out64': a convolution layer with kernel_size (3,3) input's number of channels is 3 and output's is 64\n",
        "    -'avgPool8': an average pool with kernel_size (8,8)\n",
        "    -'linear_in1024': the last linear has the number output features equal to the number of classes and input equal to 1024\n",
        "    - A Coded-ResNeXt block is described as: \n",
        "      [Number_of_Input_Channels, Number_of_Output_Channels, Bottleneck_width, Stride_Of_the_second_convolutional_layer, Coding_scheme_ration, Probability_dropSubNN ]'\n",
        "'''\n",
        "\n",
        "\n",
        "d = 11 #Bottleneck_width of first stage\n",
        "dp_prob = 0.1\n",
        "ARCHITECTURE_CIFAR_10 = [#stem. Resolution (32x32)\n",
        "                        'conv3_in3_out64', 'bn2D_in64', 'relu',\n",
        "                        #stage 1\n",
        "                        [ 64, 256, d, 1, '10/10', 0.0 ], #Index Of Block: 0,  Resolution (32x32)\n",
        "                        [ 256, 256, d, 1, '10/10', 0.0 ],\n",
        "                        [ 256, 256, d, 1, '10/10', 0.0 ],                         \n",
        "                        #stage 2\n",
        "                        [ 256, 512, 2*d, 2, '5/10' , dp_prob ], #Index Of Block: 3,  Resolution (16x16)\n",
        "                        [ 512, 512, 2*d, 1, '5/10' , dp_prob ], \n",
        "                        [ 512, 512, 2*d, 1, '5/10' , dp_prob ], \n",
        "                        #stage 3\n",
        "                        [ 512, 1024, 4*d, 2, '3/10', dp_prob ], #Index Of Block: 6,  Resolution (8x8)\n",
        "                        [ 1024, 1024, 4*d, 1, '3/10', dp_prob ], \n",
        "                        [ 1024, 1024, 4*d, 1, '3/10', dp_prob ],          \n",
        "                        #Last layers\n",
        "                        'avgPool8', 'flatten','linear_in1024' \n",
        "                        ]\n",
        "\n",
        "d=6 \n",
        "dp_prob = 0.1 \n",
        "ARCHITECTURE_CIFAR_100 = [ #stem\n",
        "                        'conv3_in3_out64', 'bn2D_in64', 'relu',\n",
        "                        #stage 1\n",
        "                        [ 64, 256, d, 1, '20/20', 0.0 ], #Index Of Block: 0,  Resolution (32x32)\n",
        "                        [ 256, 256, d, 1, '20/20',0.0 ],\n",
        "                        [ 256, 256, d, 1, '20/20',0.0 ],                         \n",
        "                        #stage 2\n",
        "                        [ 256, 512, 2*d, 2, '8/20',dp_prob ], #Index Of Block: 3,  Resolution (16x16)\n",
        "                        [ 512, 512, 2*d, 1, '8/20',dp_prob ],  \n",
        "                        [ 512, 512, 2*d, 1, '8/20',dp_prob ],  \n",
        "                        #stage 3\n",
        "                        [ 512, 1024, 4*d, 2, '4/20',dp_prob ], #Index Of Block: 6,  Resolution (16x16)\n",
        "                        [ 1024, 1024, 4*d, 1, '4/20',dp_prob ],  \n",
        "                        [ 1024, 1024, 4*d, 1, '4/20',dp_prob ],             \n",
        "                        #Last layers\n",
        "                        'avgPool8', 'flatten','linear_in1024' \n",
        "                        ]"
      ],
      "metadata": {
        "id": "rW1gX7STfAkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARGS = {\n",
        "        #~~~~~~Problem's settings~~~~~~\n",
        "        'Problem': 'Cifar10',#Choose between: 'Cifar10', 'Cifar100'  \n",
        "        'Control': False, #If True then the original ResNeXt is trained.\n",
        "\n",
        "        #~~~~~~Architectural & interpretability choices~~~~~~\n",
        "        'Energy_normalization': True,\n",
        "        'Same_code_Same_mask': True, #If True then two consecutive ResNeXt blocks that have the same coding scheme will\n",
        "                                #also have the same dropout mask applied to them. Therefore out of N consecutive ResNeXt blocks\n",
        "                                #with the same coding scheme it will be the first one dropSubNN_probability that counts.\n",
        "   \n",
        "        #~~~~~~Losses choices~~~~~~\n",
        "        'LossDisentangle_type':'power4_threshold0.0', #Loss = diff(E_subNN, target_Energy, threshold)^power.\n",
        "                                            #The diff function is: max{ |Energy_subNN-target_Energy|-threshold, 0}\n",
        "        'LossDisentanglement_coef': 4, #Coefficient the loss_disentangle is multiplied with (Denoted $\\mu$ in the paper)\n",
        "                                     #Generally it doesn't affect the overall performance but improves the binary classifiers.\n",
        "        \n",
        "        #~~~~~~RandAugment~~~~~~\n",
        "        'no-augmentation': False, #If true then no data augmentation will be used\n",
        "        'timm-AutoAugment': 'rand-m2-n1',#The notation is from \"https://fastai.github.io/timmdocs/RandAugment\"\n",
        "                                         #For CIFAR100:'rand-m2-n1' and for CIFAR10 'rand-m4-n3'\n",
        "            \n",
        "        #~~~~~~Optimization's algorithm choices~~~~~~\n",
        "        'train_batchSize': 64,#1 step in 8 core training is computing 8 gradients and syncing them, so effectively the size is 8*64=512\n",
        "        'test_batchSize': 64,\n",
        "        'N_epochs': 300, \n",
        "        'SGD_lr_momentum': [0.1, 0.9, 5e-4, True],#(initial_lr, momentum, weight decay, Nesterov)\n",
        "        'N_workers_dataloader': 4,\n",
        "        }\n",
        "\n",
        "ARGS['architecture'] =  ARCHITECTURE_CIFAR_10 if ARGS['Problem']=='Cifar10' else ARCHITECTURE_CIFAR_100"
      ],
      "metadata": {
        "id": "integWNTj08e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Also consider using your google drive to save \"permanently\" the checkpoints\n",
        "!mkdir /content/checkpoints\n",
        "SAVING_PATH_CHECKPOINTS = \"/content/checkpoints/\"\n",
        "NAME_EXPERIMENT = \"new_exp\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs=10, filename=NAME_EXPERIMENT, dirpath=SAVING_PATH_CHECKPOINTS)\n",
        "dm = DataModule(ARGS)\n",
        "my_model = Net_ResNext(ARGS)\n",
        "trainer = Trainer( max_epochs=ARGS['N_epochs'], num_sanity_val_steps=0, tpu_cores=8, precision='bf16', callbacks=[checkpoint_callback])\n",
        "\n",
        "#In the progress bar the validation accuracy also appears\n",
        "#The training/validation losses/accuracies and the accuracy of the early decoders are logged in case it is desired to \n",
        "#use the Tensorboard to plot those figures.\n",
        "trainer.fit(my_model, dm)"
      ],
      "metadata": {
        "id": "AowTf5nWgpg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After training the model  from InterpretabilityFunctions.py there are two functions to test \n",
        "the specialization of the subNNs. The first one is:\n",
        "  -test_print_acc_removing_subNNs(indx_block, List_N_subNNs_to_remove, model, device, test_loader):\n",
        "        Given the index of the block (equal to $l-1$ in reference to the paper as in the paper the indexing \n",
        "        started from 1 and here from 0) and a list indicating how many subNNs to remove (for example [2,4] will\n",
        "        do two experiments one removing 2 subNNs and one removing 4) and runs the experiment where \n",
        "        from the given block it removes with the two different ways subNNs.\n",
        "  -test_partNN_asBinaryClassifier(model, device,  class_of_model_part, test_loader, train_loader=None, out_of_distr_loader=None):\n",
        "        It tests and prints the performance of the binary classifier for the class \"class_of_model_part\". \n",
        "        Also plots the distribution of the positive, negative and out of distribution samples. It returns for that binary classifier\n",
        "        its output for all positive, negative and out of distribution samples.\n",
        "An example:\n",
        "'''\n",
        "from InterpretabilityFunctions import test_partNN_asBinaryClassifier\n",
        "import torch_xla.core.xla_model as xm\n",
        "import copy\n",
        "device = xm.xla_device()\n",
        "copied_model = Net_ResNext(ARGS)\n",
        "copied_model.load_state_dict(my_model.state_dict())\n",
        "copied_model.to(device)\n",
        "\n",
        "ARGS_OUF_OF_DISTR = copy.deepcopy(ARGS)\n",
        "ARGS_OUF_OF_DISTR['Problem'] = 'Cifar100'\n",
        "dmOutOfDistr = DataModule(ARGS_OUF_OF_DISTR)\n",
        "dmOutOfDistr.prepare_data()\n",
        "dmOutOfDistr.setup()\n",
        "\n",
        "cl = 0 #binary classifier for the first class\n",
        "res = test_partNN_asBinaryClassifier(copied_model, device, cl, dm.val_dataloader(), dm.train_dataloader(),dmOutOfDistr.val_dataloader())\n"
      ],
      "metadata": {
        "id": "vDEDvFU00F6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
