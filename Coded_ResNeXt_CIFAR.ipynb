{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coded-ResNeXt_CIFAR.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP8fMy+gFqgePskKkTbDE/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avranasa/Coded-ResNeXt/blob/main/Coded_ResNeXt_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ie-iEFQiI_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e7c07-c0ad-43f3-ce0a-83c53a4a149b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n",
            "  libdevmapper1.02.1 libfontenc1 libidn11 libip4tc0 libjansson4\n",
            "  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0 libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-dkms-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-modprobe\n",
            "  nvidia-settings openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1\n",
            "Suggested packages:\n",
            "  db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 subversion\n",
            "0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 2,237 kB of archives.\n",
            "After this operation, 9,910 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libapr1 amd64 1.6.3-2 [90.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaprutil1 amd64 1.6.1-2 [84.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserf-1-1 amd64 1.3.9-6 [44.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsvn1 amd64 1.9.7-4ubuntu1 [1,183 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 subversion amd64 1.9.7-4ubuntu1 [834 kB]\n",
            "Fetched 2,237 kB in 1s (1,776 kB/s)\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../libapr1_1.6.3-2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.6.3-2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../libaprutil1_1.6.1-2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../libserf-1-1_1.3.9-6_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../libsvn1_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../subversion_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking subversion (1.9.7-4ubuntu1) ...\n",
            "Setting up libapr1:amd64 (1.6.3-2) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-2) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Setting up libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Setting up subversion (1.9.7-4ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "A    CIFAR/Coded_ResNeXt_CIFAR.ipynb\n",
            "A    CIFAR/Helpers_InteretabilityFunctions.py\n",
            "A    CIFAR/InterpretabilityFunctions.py\n",
            "A    CIFAR/NN_ResNeXt.py\n",
            "A    CIFAR/NN_modules.py\n",
            "A    CIFAR/subNNs_codes.py\n",
            "Checked out revision 19.\n",
            "Found existing installation: torch 1.10.0+cu111\n",
            "Uninstalling torch-1.10.0+cu111:\n",
            "  Successfully uninstalled torch-1.10.0+cu111\n",
            "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
            "Collecting torch==1.8.2+cpu\n",
            "  Downloading https://download.pytorch.org/whl/lts/1.8/cpu/torch-1.8.2%2Bcpu-cp37-cp37m-linux_x86_64.whl (169.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 169.1 MB 51 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.2+cpu\n",
            "  Downloading https://download.pytorch.org/whl/lts/1.8/cpu/torchvision-0.9.2%2Bcpu-cp37-cp37m-linux_x86_64.whl (13.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.3 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cpu) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.2+cpu) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.2+cpu) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.2+cpu which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.2+cpu which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.2+cpu torchvision-0.9.2+cpu\n",
            "\u001b[K     |████████████████████████████████| 144.6 MB 105 kB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.297 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 527 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 82.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 952 kB 76.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 87.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 397 kB 79.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 77.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 81.8 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.297 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 4.6 MB/s \n",
            "\u001b[K     |██████████████▍                 | 344.5 MB 1.2 MB/s eta 0:05:43"
          ]
        }
      ],
      "source": [
        "#We had troubles on running with Colab TPU accelarator and different version of pytorch.\n",
        "#If prompted/printed after running this cell, press the button to RESTART RUNTIME \n",
        "!pip install timm\n",
        "!apt-get install -q subversion\n",
        "!svn checkout https://github.com/avranasa/Coded-ResNeXt.git/trunk/CIFAR /content/CIFAR\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu  torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q torchtext==0.9.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import os, sys, timm\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "sys.path.insert(0, '/content/CIFAR') \n",
        "from NN_ResNeXt import Net_ResNext, DataModule\n",
        "from NN_modules import ResNeXt_block\n",
        "\n",
        "'''\n",
        "For the architectures described below the notation is the following:\n",
        "    -'conv3_in3_out64': a convolution layer with kernel_size (3,3) input's number of channels is 3 and output's is 64\n",
        "    -'avgPool8': an average pool with kernel_size (8,8)\n",
        "    -'linear_in1024': the last linear has the number output features equal to the number of classes and input equal to 1024\n",
        "    - A Coded-ResNeXt block is described as: \n",
        "      [Number_of_Input_Channels, Number_of_Output_Channels, Bottleneck_width, Stride_Of_the_second_convolutional_layer, Coding_scheme_ration, Probability_dropSubNN ]'\n",
        "'''\n",
        "\n",
        "\n",
        "d = 11 #Bottleneck_width of first stage\n",
        "dp_prob = 0.1\n",
        "ARCHITECTURE_CIFAR_10 = [#stem. Resolution (32x32)\n",
        "                        'conv3_in3_out64', 'bn2D_in64', 'relu',\n",
        "                        #stage 1\n",
        "                        [ 64, 256, d, 1, '10/10', 0.0 ], #Index Of Block: 0,  Resolution (32x32)\n",
        "                        [ 256, 256, d, 1, '10/10', 0.0 ],\n",
        "                        [ 256, 256, d, 1, '10/10', 0.0 ],                         \n",
        "                        #stage 2\n",
        "                        [ 256, 512, 2*d, 2, '5/10' , dp_prob ], #Index Of Block: 3,  Resolution (16x16)\n",
        "                        [ 512, 512, 2*d, 1, '5/10' , dp_prob ], \n",
        "                        [ 512, 512, 2*d, 1, '5/10' , dp_prob ], \n",
        "                        #stage 3\n",
        "                        [ 512, 1024, 4*d, 2, '3/10', dp_prob ], #Index Of Block: 6,  Resolution (8x8)\n",
        "                        [ 1024, 1024, 4*d, 1, '3/10', dp_prob ], \n",
        "                        [ 1024, 1024, 4*d, 1, '3/10', dp_prob ],          \n",
        "                        #Last layers\n",
        "                        'avgPool8', 'flatten','linear_in1024' \n",
        "                        ]\n",
        "\n",
        "d=6 \n",
        "dp_prob = 0.1 \n",
        "ARCHITECTURE_CIFAR_100 = [ #stem\n",
        "                        'conv3_in3_out64', 'bn2D_in64', 'relu',\n",
        "                        #stage 1\n",
        "                        [ 64, 256, d, 1, '20/20', 0.0 ], #Index Of Block: 0,  Resolution (32x32)\n",
        "                        [ 256, 256, d, 1, '20/20',0.0 ],\n",
        "                        [ 256, 256, d, 1, '20/20',0.0 ],                         \n",
        "                        #stage 2\n",
        "                        [ 256, 512, 2*d, 2, '8/20',dp_prob ], #Index Of Block: 3,  Resolution (16x16)\n",
        "                        [ 512, 512, 2*d, 1, '8/20',dp_prob ],  \n",
        "                        [ 512, 512, 2*d, 1, '8/20',dp_prob ],  \n",
        "                        #stage 3\n",
        "                        [ 512, 1024, 4*d, 2, '4/20',dp_prob ], #Index Of Block: 6,  Resolution (16x16)\n",
        "                        [ 1024, 1024, 4*d, 1, '4/20',dp_prob ],  \n",
        "                        [ 1024, 1024, 4*d, 1, '4/20',dp_prob ],             \n",
        "                        #Last layers\n",
        "                        'avgPool8', 'flatten','linear_in1024' \n",
        "                        ]"
      ],
      "metadata": {
        "id": "rW1gX7STfAkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARGS = {\n",
        "        #~~~~~~Problem's settings~~~~~~\n",
        "        'Problem': 'Cifar10',#Choose between: 'Cifar10', 'Cifar100'  \n",
        "        'Control': False, #If True then the original ResNeXt is trained.\n",
        "\n",
        "        #~~~~~~Architectural & interpretability choices~~~~~~\n",
        "        'Energy_normalization': True,\n",
        "        'Same_code_Same_mask': True, #If True then two consecutive ResNeXt blocks that have the same coding scheme will\n",
        "                                #also have the same dropout mask applied to them. Therefore out of N consecutive ResNeXt blocks\n",
        "                                #with the same coding scheme it will be the first one dropSubNN_probability that counts.\n",
        "   \n",
        "        #~~~~~~Losses choices~~~~~~\n",
        "        'LossDisentangle_type':'power4_threshold0.0', #Loss = diff(E_subNN, target_Energy, threshold)^power.\n",
        "                                            #The diff function is: max{ |Energy_subNN-target_Energy|-threshold, 0}\n",
        "        'LossDisentanglement_coef': 4, #Coefficient the loss_disentangle is multiplied with (Denoted $\\mu$ in the paper)\n",
        "                                     #Generally it doesn't affect the overall performance but improves the binary classifiers.\n",
        "        \n",
        "        #~~~~~~RandAugment~~~~~~\n",
        "        'no-augmentation': False, #If true then no data augmentation will be used\n",
        "        'timm-AutoAugment': 'rand-m2-n1',#The notation is from \"https://fastai.github.io/timmdocs/RandAugment\"\n",
        "                                         #For CIFAR100:'rand-m2-n1' and for CIFAR10 'rand-m4-n3'\n",
        "            \n",
        "        #~~~~~~Optimization's algorithm choices~~~~~~\n",
        "        'train_batchSize': 64,#1 step in 8 core training is computing 8 gradients and syncing them, so effectively the size is 8*64=512\n",
        "        'test_batchSize': 64,\n",
        "        'N_epochs': 300, \n",
        "        'SGD_lr_momentum': [0.1, 0.9, 5e-4, True],#(initial_lr, momentum, weight decay, Nesterov)\n",
        "        'N_workers_dataloader': 4,\n",
        "        }\n",
        "\n",
        "ARGS['architecture'] =  ARCHITECTURE_CIFAR_10 if ARGS['Problem']=='Cifar10' else ARCHITECTURE_CIFAR_100"
      ],
      "metadata": {
        "id": "integWNTj08e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Also consider using your google drive to save \"permanently\" the checkpoints\n",
        "!mkdir /content/checkpoints\n",
        "SAVING_PATH_CHECKPOINTS = \"/content/checkpoints/\"\n",
        "NAME_EXPERIMENT = \"new_exp\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(every_n_epochs=10, filename=NAME_EXPERIMENT, dirpath=SAVING_PATH_CHECKPOINTS)\n",
        "dm = DataModule(ARGS)\n",
        "my_model = Net_ResNext(ARGS)\n",
        "trainer = Trainer( max_epochs=ARGS['N_epochs'], num_sanity_val_steps=0, tpu_cores=8, precision='bf16', callbacks=[checkpoint_callback])\n",
        "\n",
        "#In the progress bar the validation accuracy also appears\n",
        "#The training/validation losses/accuracies and the accuracy of the early decoders are logged in case it is desired to \n",
        "#use the Tensorboard to plot those figures.\n",
        "trainer.fit(my_model, dm)"
      ],
      "metadata": {
        "id": "AowTf5nWgpg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After training the model  from InterpretabilityFunctions.py there are two functions to test \n",
        "the specialization of the subNNs. The first one is:\n",
        "  -test_print_acc_removing_subNNs(indx_block, List_N_subNNs_to_remove, model, device, test_loader):\n",
        "        Given the index of the block (equal to $l-1$ in reference to the paper as in the paper the indexing \n",
        "        started from 1 and here from 0) and a list indicating how many subNNs to remove (for example [2,4] will\n",
        "        do two experiments one removing 2 subNNs and one removing 4) and runs the experiment where \n",
        "        from the given block it removes with the two different ways subNNs.\n",
        "  -test_partNN_asBinaryClassifier(model, device,  class_of_model_part, test_loader, train_loader=None, out_of_distr_loader=None):\n",
        "        It tests and prints the performance of the binary classifier for the class \"class_of_model_part\". \n",
        "        Also plots the distribution of the positive, negative and out of distribution samples. It returns for that binary classifier\n",
        "        its output for all positive, negative and out of distribution samples.\n",
        "An example:\n",
        "'''\n",
        "from InterpretabilityFunctions import test_partNN_asBinaryClassifier\n",
        "import torch_xla.core.xla_model as xm\n",
        "import copy\n",
        "device = xm.xla_device()\n",
        "copied_model = Net_ResNext(ARGS)\n",
        "copied_model.load_state_dict(my_model.state_dict())\n",
        "copied_model.to(device)\n",
        "\n",
        "ARGS_OUF_OF_DISTR = copy.deepcopy(ARGS)\n",
        "ARGS_OUF_OF_DISTR['Problem'] = 'Cifar100'\n",
        "dmOutOfDistr = DataModule(ARGS_OUF_OF_DISTR)\n",
        "dmOutOfDistr.prepare_data()\n",
        "dmOutOfDistr.setup()\n",
        "\n",
        "cl = 0 #binary classifier for the first class\n",
        "res = test_partNN_asBinaryClassifier(copied_model, device, cl, dm.val_dataloader(), dm.train_dataloader(),dmOutOfDistr.val_dataloader())\n"
      ],
      "metadata": {
        "id": "vDEDvFU00F6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}